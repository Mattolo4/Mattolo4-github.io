<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <base href="../../../">

    <title>Obstacle Detection</title>
    <link rel="icon" type="image/x-icon" href="assets/img/portfolio/Icon/favicon.png">
    <meta content="Obstacle detectiom routine for the Robot Tiago" name="description">
    <meta content=" " name="keywords">
    <meta content="Matteo Villani" name="Author">

    <!-- Google Fonts -->
    <link
        href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
        rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="assets/vendor/aos/aos.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

        <!-- Chart -->
    <script src="assets/js/chart.js"></script>

    <!-- Random color generator -->
    <script src="assets/js/randomColors.js"></script>


    <!-- LaTeX in HTML-->
    <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script> -->

    <!-- Template Main CSS File -->
    <link href="assets/css/style.css" rel="stylesheet">

</head>

<!-- Visit Counter section -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9QT0BDN1XE"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9QT0BDN1XE');
</script>

<body>
    <!-- Mobile nav toggle button -->
    <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>
    <span id="top"></span>

    <!-- Header -->
    <header id="header">
        <div class="d-flex flex-column">

            <div class="profile">
                <!--for better resolution check the img size-->
                <img src="assets/img/profile-img2-sqaured.jpg" alt="Profile Image" class="img-fluid rounded-circle">
                <h1 class="text-light"><a href="index.html">Matteo Villani</a></h1>

                <div class="social-links mt-3 text-center">
                    <a href="mailto:villanimatteo33@gmail.com" class="email"><i class="bi bi-envelope-fill"></i></a>
                    <a href="https://github.com/Mattolo4?tab=repositories" class="github"><i
                            class="bx bxl-github"></i></a>
                    <a href="https://www.instagram.com/matto.lo4/" class="instagram"><i
                            class="bx bxl-instagram"></i></a>
                    <a href="https://www.linkedin.com/in/matteo-villani-851ba5199/" class="linkedin"><i
                            class="bx bxl-linkedin"></i></a>
                </div>
            </div>

            <!--Nav menu-->
            <nav id="navbar" class="nav-menu navbar">
                <ul>
                    <li><a href="#hero" class="nav-link scrollto active"><i class="bx bx-home"></i>
                            <span>Home</span></a></li>
                    <li><a href="#about" class="nav-link scrollto"><i class="bx bx-user"></i> 
                            <span>About</span></a></li>
                    <li><a href="#skills" class="nav-link scrollto"><i class="bi bi-diagram-3"></i>
                            <span>Skills</span></a></li>
                    <li><a href="#portfolio" class="nav-link scrollto"><i class="bx bx-book-content"></i>
                            <span>Portfolio</span></a></li>
                    <li><a href="#resume" class="nav-link scrollto"><i class="bx bx-file-blank"></i>
                            <span>Resume</span></a></li>
                    <li><a href="#interests" class="nav-link scrollto"><i class="bx bx-server"></i>
                            <span>Interests</span></a></li>
                    <li><a href="#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i>
                            <span>Contacts</span></a></li>
                </ul>
            </nav>
        </div>
    </header>
    <!-- END header -->



    <main id="main">

        <!-- Breadcrumbs -->
        <section id="breadcrumbs" class="breadcrumbs">
            <div class="container">

                <div class="d-flex justify-content-between align-items-center">
                    <h2>Obstacle detection for the Robot Tiago</h2>
                    <ol>
                        <li><a href="index.html">Home</a></li>
                        <li>Obstacle-detection</li>
                    </ol>
                </div>

            </div>
        </section><!-- End Breadcrumbs -->

        <!-- Portfolio Details Section -->
        <section id="portfolio-details" class="portfolio-details">
            <div class="container about">

                <div class="row gy-4">
                    <div class="col-lg-7 align-vert">
                        <div class="portfolio-details-slider swiper">
                            <div class="swiper-wrapper align-items-center">

                                <div class="swiper-slide">
                                    <img src="assets/img/Projects/Obstacle-detection/Poster.png" title="Poster" alt="Poster">
                                </div>
                                <div class="swiper-slide">
                                    <img src="assets\img\Projects\Obstacle-detection\potential fields summary.png" title="Potential fields summary" alt="Potential fields summary">
                                </div>
                                <div class="swiper-slide">
                                    <img src="assets\img\Projects\Obstacle-detection\raw laser readings.png" title="Raw laser readings" alt="Raw laser readings" width="550" height="271">
                                </div>
                                <div class="swiper-slide">
                                    <img src="assets\img\Projects\Obstacle-detection\1st derivative.png" title="1st derivative" alt="1st derivative" width="550" height="271">
                                </div>
                                <div class="swiper-slide">
                                    <img src="assets\img\Projects\Obstacle-detection\laser scan tresholded.png" title="Laser scan tresholded" alt="Laser scan tresholded" width="550" height="271">
                                </div>
                                <div class="swiper-slide">
                                    <img src="assets\img\Projects\Obstacle-detection\filled laser scan vector.png" title="Filled laser scan vector" alt="Filled laser scan vector" width="550" height="271">
                                </div>

                            </div>
                            <div class="swiper-pagination"></div>
                        </div>
                    </div>

                    <div class="col-lg-4">
                        <div class="portfolio-info">
                            <h3>Project information</h3>
                            <ul>
                                <li><strong>Category</strong>: Intelligent Robot Software</li>
                                <li><strong>Project date</strong>: Dec, 2023</li>
                                <li><strong>Client</strong>: Accademic project
                                <li><strong>GitHub</strong>: <a href="https://github.com/Mattolo4/Obstacles-Detection-for-Robot-Tiago">Source code</a></li>
                                <li><strong>Main tool</strong>: <a href="https://www.ros.org/">ROS</a> - <a href="https://wiki.ros.org/rviz">Rviz</a> - <a href="https://gazebosim.org/home">Gazebo</a></li>
                            </ul>
                        </div>

                        <div class="d-flex text-center pt-3" style="height: 40vh; justify-content: center;">
                            <canvas id="chart"></canvas>
                        </div>

                        <script>
                            const info = [81.8, 18.2];
                            const lang = ['C++', 'Cmake'];

                            const colors = generateDistinctColors(info.length);
                            const ctx = document.getElementById('chart');
                            console.log(colors);

                            const data = {
                                datasets: [{
                                    data: info,
                                    borderWidth: 3,
                                    backgroundColor: colors,
                                }],
                                labels: lang,
                            }

                            new Chart(ctx, {
                                type: 'doughnut',
                                data: data,
                                options: {
                                    responsive: true,
                                    plugins: {
                                        title: {
                                            display: true,
                                            text: 'Languages',
                                        }
                                    },
                                }
                            })
                        </script>
                    </div>
                </div>

                <div class="content" data-aos="fade-left" data-aos-deley="100">
                    <h2>Description</h2>
                    <p>The goal of the project is to get the robot Tiago from the initial point <b>Starting Pose</b> to point <b>PoseB</b> by navigating
                    through the environment that includes two rooms, corridor and obstacles.</p>
                    <p>Once Tiago reaches the PoseB, it must recognize the obstacles, cylindrical in shape, from his surroundings and indicate
                    how many there are and their location in the room. Tiago can reach PoseB in three different ways:</p>
                    <ol>
                        <li>The first involves using the motion control low that we developed to reach PoseB directly, thus never using the
                        <b>Navigation stack</b></li>
                        <li>The second implies an <b>hybrid control</b>: involves using a <u>motion control low</u> that we developed to cross the corridor and
                        then using the <u>Navigation stack</u> to reach PoseB</li>
                        <li>The third involves using the control low that is already there by taking advantage of the <b>Navigation stack</b>.</li>
                    </ol>
                </div>

                <div class="content pt-4 py-2" data-aos="fade-left" data-aos-deley="150">
                    <h2>Structure</h2>
                    <p>We implement an Action Client/Server structure:</p>
                    <ul>
                        <li><i class="bi bi-chevron-right"></i><b class="px-1">Client: </b> the action client receives the input from the user by command line, by providing:
                            the x, y coordinates regarding location and yaw regarding orientation. This information will go to create PoseB that
                            corresponds to our goal. And which of the three possible motion control laws the user want to use choosing between 1, 2, 3.
                        </li>
                    </ul>
                    <p>Once the goal is obtained, the client sends the information to the server. The action client also implements <b>callbacks</b>
                    to the <b>feedback</b> of the action server, and prints the task's current status in the terminal.</p>
                    <ul>
                        <li><i class="bi bi-chevron-right"></i><b class="px-1">Server: </b> the action server executes all the tasks.</li>
                    </ul>

                    <ol>
                        <li>First it acts as a client for the move action, used to reach our goal, sending the information to the dedicated server
                        that will process the robot's movement, in accordance with the mode chosen as the <b>motion control law</b> to achieve the goal
                        (1, 2, 3).</li>

                        <li class="py-2">Once the goal is reached, it subscribes to the topic <b>/scan</b> to get the information regarding the surroundings area. This
                        information will then be used for locating objects and counting them.</li>

                        <li>When all the information is obtained, the server returns it to the client indicating the <b>location</b> (in the ”base laser
                        link” reference frame) and <b>quantity</b> of obstalces in the room.</li>
                    </ol>

                    <div class="row-lg-11 text-center">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\structure.png" alt="Structure"
                                class="img-fluid figure-img rounded" width="550" height="281">
                        </figure>
                    
                        <figcaption class="figure-caption">Software structure</figcaption>
                    </div>
                </div>

                <div class="content pt-4 py-2" data-aos="fade-left" data-aos-deley="200">
                    <h2>Objects detection</h2>
                    <p>Our approach takes into account the following <b>assumptions</b>: the objects to be detected should be <i>detached from the wall</i>
                    and <i>cylindrical</i> in shape. Informations obtained from the laser scan present on Tiago was used to detect obstacles.</p>

                    <div class="row-lg-11 text-center">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\raw laser readings.png" alt="Raw laser data"
                                class="img-fluid figure-img rounded" width="550" height="330">
                        </figure>
                    
                        <figcaption class="figure-caption">Raw Laser readings</figcaption>
                    </div>

                    <p>The approach is as follows:</p>

                    <ul>
                        <li><i class="bi bi-chevron-right"></i>Using the distance data associated with a certain angle obtained from the laser scan, we calculate the derivative
                        (understood as the difference of two consecutive values: [i]-[i-1])to detect discontinuities. This works like an edge
                        detector in image processing, detecting the right edge discontinuities in the laser scan since the scan is counter
                        clockwise. Such discontinuities, in fact, highlight the presence of a different object, since the difference of two
                        consecutive points will be a very small value if they belong to the same element (e.g., the wall).
                        Saving the obtained values in a vector having the length equal to the laser data vector.</li>
                    </ul>
                    <div class="row-lg-11 text-center py-2">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\1st derivative.png" alt="1st derivative"
                                class="img-fluid figure-img rounded" width="550" height="271">
                        </figure>
                    
                        <figcaption class="figure-caption">1st derivative</figcaption>
                    </div>
                    <ul>
                        <li><i class="bi bi-chevron-right"></i>Once we have obtained the vector of discontinuities, we check vector values and remove ones that are too small, choosing a threshold value of 0.5m proved to be a good value to distinguish objects and walls.
                        In this way, we get a vector where most of the values are zero, except for the indexes where we detect the right-edge of an object.</li>
                    </ul>
                    <div class="row-lg-11 text-center py-2">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\laser scan tresholded.png" alt="Tresholded derivative (0.5)"
                                class="img-fluid figure-img rounded" width="550" height="272">
                        </figure>
                    
                        <figcaption class="figure-caption">Tresholded derivative (0.5)</figcaption>
                    </div>

                    <ul>
                        <li><i class="bi bi-chevron-right"></i>Then this vector was used as a guideline to create another vector of only the data points that correspond to objects,
                        this was accomplished by taking the original distance value (corresponding to the right-edge index of object) and
                        keeping the following distance values until they are within a threshold in a counter-clockwise sweeping motion. We then
                        obtain a vector with all zero values but the distance values corresponding to the various objects.</li>
                    </ul>   

                    <div class="row-lg-11 text-center py-2">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\filled laser scan vector.png" alt="Filled laser scan vector"
                                class="img-fluid figure-img rounded" width="550" height="271">
                        </figure>
                    
                        <figcaption class="figure-caption">Filled laser scan vector</figcaption>
                    </div>
                    <ul>
                        <li><i class="bi bi-chevron-right"></i>To find the center of the objects, knowing it is of a cylindrical shape, we choose to take the initial, halfway, final
                        points of the singular objects and interpolate a circle. From the interpolated circle we then obtain the center point
                        coordinates with respect to the base-laser-link-frame.</li>
                    </ul>

                    <div class="row-lg-11 text-center py-2">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\center circle.png" alt="Obstacle's center computation"
                                class="img-fluid figure-img rounded" width="369" height="329">
                        </figure>
                    
                        <figcaption class="figure-caption">Obstacle's center computation</figcaption>
                    </div>
                    <p>This approach also <b>ignores</b> all points too close to the sensor, such as parts of the robot itself.
                    The detection was tested in various position on the map and was found to be successful in detecting the visible object
                    and obtaining the correct center-point values.</p>
                </div>

                <div class="content pt-4 py-1">
                    <h2>Motion Control Law</h2>
                    <p>This task was to implement a <b>motion control law</b> to reach a goal point that directly process the sensor data and generate
                    velocity command for the robot in the <i>mobile_base_controller/cmd_ve</i>l without using the <i>move_base stack</i>.</p>

                    <p>To achieve this, the idea was similar to the <b>potential fields</b> algorithm but implemented in a <b>reactive and radial form.</b>
                    We computed two vectors, an <b>attractive force vector</b> and a <b>repulsive force vector</b> which are then summed to create the
                    <b>total force vector</b>. This vectors have a size equal to a radius <i>2π×angle_increment</i> to obtain a vector size of 1088. In
                    this way the <b>robot X-axis</b> direction is the halfway point of the array, the points after this are the positive angles <b>+θ</b>
                    and the ones before are the negative angle <b>−θ</b>.</p>

                    <ul>
                        <li><i class="bi bi-chevron-right"></i>The attractive force vector was computed by first obtaining the index of the bin correspondingto the Goal heading angle,
                        then the vector was compiled by assigning a value of 1 to this index and a linearly decreasing value for all other more
                        distant indexes.</li>
                    </ul>
                    <p><u>This means the closer you are pointing to the goal heading angle the higher attractive force value.</u></p>
                    <ul>
                        <li><i class="bi bi-chevron-right"></i>The repulsive force vector was instead computed using the LaserScan message from the scanand by setting the repulsive
                        force to 0 if nothing was detected or was out of the laser sight and setting it to 1−g×range[i] if anything was
                        detected, where g is the potential repulsive gain that can be tuned.</li>

                        <li><i class="bi bi-chevron-right"></i>The total force vector was then computed summing the two previous force vectors.</li>
                    </ul>

                    <div class="row-lg-11 text-center py-2">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\Attractive_potential.png" alt="Attractive potential"
                                class="img-fluid figure-img rounded" width="430" height="317">
                            <img src="assets\img\Projects\Obstacle-detection\Repulsive_potential.png" alt="Repulsive potential"
                                class="img-fluid figure-img rounded" width="430" height="242">
                            <img src="assets\img\Projects\Obstacle-detection\Total_potential.png" alt="Totaol potential"
                                class="img-fluid figure-img rounded" width="430" height="281">
                    
                        </figure>
                        <figcaption class="figure-caption">Attractive - Repulsive - Total potentials</figcaption>
                    </div>

                    <p>From the total force vector we can select the index corresponding to <b>highest potential</b> which is then used as the angular
                    direction for the robot to follow to reach the goal and avoid collisions.</p>

                    <p>To improve this rudimental approach many steps of <i>pre-processing</i> were implemented.</p>

                    <p>The LaserScan data was <b>smoothed</b> within the 8 nearest neighbours so that the fall-off from object corners was gradual,
                    this is needed because the robot thinks itself as a point and may believe to be able topass close to a corner without
                    hitting it. Then an obstacle detection function was implemented which takes a <b>cone</b> in front of the robot as detection
                    area where,
                    if anything is detected inside this area, the repulsive force for the corresponding bins are <b>incremented</b> depending
                    on the freespace left.</p>

                    <p>Also a <b>fail-safe function</b> was implemented so that if anything was detected in a ”dangerzone” (which is a semicircle of
                    radius 0.5 from the robot and <i>±70◦</i> from the X-axis) the robot will <b>rotate until it clears this obstacle.</b> </p>

                    <p>After obtaining the safe best heading angle direction the <b>velocity commands are published</b> in the appropriate topic as a
                    <i>geometry_msgs::Twist_message</i> which contains the linear velocity in the X-axis and the angular velocity in the Z-axis.</p>

                    <p>The linear velocity was computed as: <i>v∗(π−maxPotAngle)/(π∗frontRange/2)</i> where <i>v</i> is the <b>linear velocity gain</b>,
                    <i>maxPotAngleis</i> the chosen <b>angle with max potential</b> and <i>frontRange</i> is the <b>laser detection in front of the robot.</b></p>

                    <p>This results in a dynamic linear velocity that let the robot travel fast when nothing is in front of it and slow down
                    when it has to do a turn. The angular speed was instead computed as : <i>(maxPotAngle∗angleVelGain)/π</i> where <i>angleVelGain</i> is a <b>tunable gain</b> value for
                    the angle velocity.</p>
                </div>


                <div class="content pt-4 py-2" data-aos="fade-left" data-aos-deley="250">
                    <h2>Results</h2>

                    <p>In all three modes 1, 2, 3 once the goal is reached, the detection of the number of obstacles iscorrect and equal to
                    <i>four</i> (according to our simulation).</p>

                    <p>Even when testing the detection where an obstacle was completely occluded three objects where correctly detected. The
                    position of the centers of the cylindrical objects calculated by us corresponds to the correct one displayed through
                    <a href="https://wiki.ros.org/rviz"><b>Rviz</b></a>. This was tested on different goals and was successful in all cases.</p>
                  
                    <div class="row-lg-11 text-center py-2">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\client.png" alt="Client terminal"
                                class="img-fluid figure-img rounded" width="600" height="264">
                    
                        </figure>
                        <figcaption class="figure-caption">Client terminal</figcaption>
                    </div>

                    <div class="row-lg-11 text-center py-2">
                        <figure class="figure">
                            <img src="assets\img\Projects\Obstacle-detection\result.png" alt="Results terminal + Simulation (Gazeboo) + LaserData (Rviz)"
                                class="img-fluid figure-img rounded" width="900" height="508">
                    
                        </figure>
                        <figcaption class="figure-caption">Results terminal + Simulation (Gazeboo) + LaserData (Rviz)</figcaption>
                    </div>

                    <div class="content py-1">
                    <p>As usual feel free to check the <a href="https://github.com/Mattolo4/Obstacles-Detection-for-Robot-Tiago" title="GitHub repo">GitHub repo <i class="bi bi-github"></i></a>.</p>
                </div>

                </div>
            </div>
        </section><!-- End Portfolio Details Section -->
    </main><!-- End #main -->

    <!-- Footer -->
    <footer id="footer">
        <div class="container">
            <div class="copyright">
                &copy; Copyright <strong><span>iPortfolio</span></strong>
            </div>
            <div class="credits">
                <!-- Licensing information: https://bootstrapmade.com/license/ -->
                Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
            </div>
        </div>
    </footer><!-- End  Footer -->

    <a href="assets/html/Projects/obstacle-detection.html#top"
        class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
    <script src="assets/vendor/aos/aos.js"></script>
    <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="assets/vendor/typed.js/typed.min.js"></script>
    <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
    <script src="assets/vendor/php-email-form/validate.js"></script>

    <!-- Template Main JS File -->
    <script src="assets/js/main.js"></script>

</body>

</html>